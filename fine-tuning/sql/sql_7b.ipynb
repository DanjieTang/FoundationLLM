{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f423a45-101c-43de-bd22-73575b73df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb07f0-21f8-4abb-95e2-9ddb03ae1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c18e8a-f393-4632-86c5-a5f8e959ff92",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d26eec-4939-4595-aea8-c0456a4d3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_token = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb16a31-4359-44f9-bf19-ecb684582293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # gpt2 does not have default padding token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f0a05-5291-431a-9478-e96e38c15c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_conversation(row):\n",
    "    return {\"complete_conversation\": \"Question: \" +row[\"question\"] + \"\\nContext:\" + row[\"context\"] + \"\\nAnswer\" + row[\"answer\"] + \"</s>\"}\n",
    "\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(row[\"complete_conversation\"], max_length=max_token, truncation=False, padding=\"max_length\")\n",
    "\n",
    "def is_shorter_than_max_token(row):\n",
    "    return len(row['input_ids']) <= max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6938dc-e381-4eb4-b7da-f41c76778e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and tokenize dataset\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "dataset = dataset['train'].train_test_split(test_size=0.05)\n",
    "\n",
    "# Turn each row into one sentence\n",
    "dataset = dataset.map(lambda x: complete_conversation(x))\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = dataset.map(lambda x: tokenize_function(x))\n",
    "\n",
    "# Filter conversation longer than token limit\n",
    "dataset = dataset.filter(is_shorter_than_max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24ab26-f14f-44cd-80e1-c818a8f5f193",
   "metadata": {},
   "source": [
    "# Load model and preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eedfe-fa2d-4a23-a2bb-a1093b82ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7266c-a9bb-411e-8307-e7e3b582cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA config\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, #alpha scaling\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a2d2f-45e2-443b-819a-2b5ce2bc16b8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5526fa9-f92f-489f-b929-c3efe4b5578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir\",\n",
    "    per_device_train_batch_size=20,\n",
    "    gradient_accumulation_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=1e-3,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8e82b-0bd3-45d7-aac6-a868eca675a0",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64db57-787a-4327-91a6-aee032459aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_query(question: str, context: str) -> str:\n",
    "    input = \"Question: \" + question + \"\\nContext:\" + context + \"\\nAnswer\"\n",
    "    \n",
    "    # Encode and move tensor into cuda if applicable.\n",
    "    encoded_input = tokenizer(input, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "    output = model.generate(**encoded_input, max_new_tokens=256)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(input):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08a474-89ad-4fc3-b0fa-c4028788c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sql_query(\"What is the highest age of users with name Danjie\", \"CREATE TABLE user (age INTEGER, name STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e971d-ff8e-4813-acb6-6194467747af",
   "metadata": {},
   "source": [
    "## Push the model to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b399-67b2-4e67-a11e-27c69033f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Danjie/SQLMaster\", commit_message=\"first draft\", private=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55697a9a-b32e-4abf-b19d-1b0264b812a1",
   "metadata": {},
   "source": [
    "## Load the model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b6da2-f6de-479d-a9c4-38d261231866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"Danjie/SQLMaster\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
