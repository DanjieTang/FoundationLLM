{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install nltk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOzDPUPKjC0W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from torch.cuda import amp\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_token = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 10\n",
    "batch_size = 300\n",
    "validation_batch_size = 10\n",
    "weight_decay = 1e-3\n",
    "lr = 1e-3\n",
    "num_layer = 1\n",
    "gradient_accumulation_step = 1\n",
    "head_dim = 128\n",
    "expansion_factor = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load llama2 and use its tokenizer and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "tokenizer.pad_token_id = 32000\n",
    "\n",
    "# Access the embedding matrix\n",
    "word_embeddings_tensor = model.model.embed_tokens.weight.data\n",
    "\n",
    "# Delete llama2 becase we are no longer using it.\n",
    "del model\n",
    "\n",
    "# Add in token embedding for pad token\n",
    "pad_token = torch.zeros(1, word_embeddings_tensor.shape[-1]).to(device)\n",
    "word_embeddings_tensor = torch.cat([word_embeddings_tensor, pad_token], dim=0)\n",
    "\n",
    "# Store vocabulary size and embedding dimension\n",
    "num_embeddings, embedding_dim = word_embeddings_tensor.shape\n",
    "word_embeddings_tensor.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pt file (If you had already preprocessed)\n",
    "word_embeddings_tensor = torch.load('word_embeddings_tensor.pt')\n",
    "num_embeddings, embedding_dim = word_embeddings_tensor.shape\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if you want to do data preprocessing\n",
    "tokenized_data = []\n",
    "attention_data = []\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", trust_remote_code=True)\n",
    "training_dataset = dataset[\"train\"]\n",
    "\n",
    "# Tokenize all training data and filter those longer than token limit\n",
    "for i in tqdm(range(len(training_dataset))):\n",
    "    text = training_dataset[i][\"text\"]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [sentence+\"</s>\" for sentence in sentences]\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenized_sentence = tokenizer(sentences, padding='max_length', max_length=max_token)\n",
    "    input_ids = tokenized_sentence[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentence[\"attention_mask\"]\n",
    "\n",
    "    # Filter those longer than max_token\n",
    "    for j in range(len(input_ids)):\n",
    "        if len(input_ids[j]) <= max_token:\n",
    "            tokenized_data.append(input_ids[j])\n",
    "            attention_data.append(attention_mask[j])\n",
    "            \n",
    "    if i == 200000:\n",
    "        break\n",
    "\n",
    "# Write into json\n",
    "with open('tokenized_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(tokenized_data, file)\n",
    "\n",
    "with open('attention_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(attention_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from json (If you had already preprocessed)\n",
    "with open('tokenized_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    tokenized_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_num = len(tokenized_data)\n",
    "training_data_num = int(total_data_num * 0.98)\n",
    "\n",
    "training_data = torch.tensor(tokenized_data[:training_data_num])\n",
    "validation_data = torch.tensor(tokenized_data[training_data_num:])\n",
    "\n",
    "# Create a TensorDataset\n",
    "training_data = TensorDataset(training_data)\n",
    "validation_data = TensorDataset(validation_data)\n",
    "\n",
    "# Use DataLoader for batching, etc.\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "\n",
    "# Free up memory\n",
    "del tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ommkUOkjG6r"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6, wandb: bool = False):\n",
    "        super().__init__()\n",
    "        self.sqrt_dim: float = 1 / math.sqrt(dim)\n",
    "        self.eps: float = eps\n",
    "        self.wandb: bool = wandb\n",
    "        if wandb:\n",
    "            self.scale = nn.Parameter(torch.ones(dim))\n",
    "            self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def find_rms_value(self, tensor: torch.Tensor) -> float:\n",
    "        norm_2 = tensor.norm(2, dim=-1)\n",
    "        return norm_2 * self.sqrt_dim\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = tensor.float() # Using 4 bit float for stability\n",
    "        rms: float = self.find_rms_value(tensor)\n",
    "        tensor = tensor/(rms.unsqueeze(-1) + self.eps)\n",
    "\n",
    "        if self.wandb:\n",
    "            tensor = tensor * self.scale\n",
    "            tensor = tensor + self.bias\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class ROPEEmbedding(nn.Module):\n",
    "    def __init__(self, max_token: int, dim: int, theta: int):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.create_embedding(max_token, dim, theta)\n",
    "\n",
    "    def create_embedding(self, max_token: int, dim: int, theta: int) -> torch.Tensor:\n",
    "        tensor = torch.arange(0, dim // 2)\n",
    "        tensor = torch.repeat_interleave(tensor, 2)\n",
    "        tensor = -tensor * 2 / dim\n",
    "        tensor = torch.pow(theta, tensor)\n",
    "\n",
    "        index = torch.arange(max_token).float() # This is the m in the formula\n",
    "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
    "\n",
    "        cos_matrix = tensor.cos()\n",
    "        sin_matrix = tensor.sin()\n",
    "        sin_matrix[0::2] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
    "\n",
    "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
    "        pos_emb = pos_emb.transpose(1, 0)\n",
    "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def flip_for_sin(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        original_shape = tensor.shape\n",
    "        tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
    "        tensor = tensor[..., [1, 0]] # Swap\n",
    "        tensor = tensor.reshape(original_shape) # Get back to original shape\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
    "\n",
    "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
    "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
    "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
    "        tensor = cos + sin\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, head_dim: int, q_head: int, kv_head: int, embedding: ROPEEmbedding):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.embedding = embedding\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "\n",
    "        tensor = self.qkv(tensor)\n",
    "        query, key, value = tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Apply ROPE\n",
    "        query = self.embedding(query)\n",
    "        key = self.embedding(key)\n",
    "        \n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int, inner_size: int):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, inner_size * 2)\n",
    "        self.down = nn.Linear(inner_size, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = self.gate_and_up(tensor)\n",
    "        gate, up = tensor.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.down(tensor)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class GemmaLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, inner_size: int, head_dim: int, q_head: int, kv_head: int, embedding: ROPEEmbedding):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(hidden_dim)\n",
    "        self.mqa = MultiQueryAttention(hidden_dim, head_dim, q_head, kv_head, embedding)\n",
    "\n",
    "        self.norm2 = RMSNorm(hidden_dim)\n",
    "        self.ffn = FeedForward(hidden_dim, inner_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.mqa(tensor, attention_mask)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        tensor = self.ffn(tensor)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor\n",
    "\n",
    "class Gemma(nn.Module):\n",
    "    def __init__(self, num_layer: int, vocab_size: int, max_token: int, hidden_dim: int, inner_size: int, head_dim: int, q_head: int, kv_head: int, theta: int = 10000):\n",
    "        super().__init__()\n",
    "        self.embedding = ROPEEmbedding(max_token, head_dim, theta)\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(GemmaLayer(hidden_dim, inner_size, head_dim, q_head, kv_head, self.embedding))\n",
    "        self.output_norm = RMSNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = tensor.shape[1]\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(device)\n",
    "        for layer in self.transformer:\n",
    "            tensor = layer(tensor, causal_mask)\n",
    "\n",
    "        tensor = self.output_norm(tensor)\n",
    "\n",
    "        # Classification\n",
    "        tensor = self.classifier(tensor)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = Gemma(num_layer, num_embeddings, max_token, embedding_dim, embedding_dim*expansion_factor, head_dim, embedding_dim//head_dim, embedding_dim//head_dim).to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in gemma.parameters()), \"parameters.\")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(gemma.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        # Teacher forcing\n",
    "        input_data = data[0][:, :-1].to(device)\n",
    "        target_data = data[0][:, 1:].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = gemma(input_embeddings)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "\n",
    "            mask = target_data != 32000\n",
    "            prediction = prediction[mask]\n",
    "            target_data = target_data[mask]\n",
    "\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "            scaler.scale(loss/gradient_accumulation_step).backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_idx + 1) % gradient_accumulation_step == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            torch.nn.utils.clip_grad_norm_(gemma.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # Clear out grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(validation_loader):\n",
    "            # Teacher forcing\n",
    "            input_data = data[0][:, :-1].to(device)\n",
    "            target_data = data[0][:, 1:].to(device)\n",
    "    \n",
    "            # Convert to embedding.\n",
    "            input_embeddings = word_embeddings_tensor[input_data]\n",
    "    \n",
    "            # Forward pass\n",
    "            with amp.autocast():\n",
    "                prediction = gemma(input_embeddings)\n",
    "    \n",
    "                # Change shape for loss calculation\n",
    "                prediction = prediction.view(-1, num_embeddings)\n",
    "                target_data = target_data.reshape(-1)\n",
    "\n",
    "                mask = target_data != 32000\n",
    "                prediction = prediction[mask]\n",
    "                target_data = target_data[mask]\n",
    "                    \n",
    "                loss = criterion(prediction, target_data) # Calculate loss\n",
    "    \n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "    \n",
    "        loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    print(\"Training loss: \", loss_train[-1])\n",
    "    print(\"Validation loss: \", loss_valid[-1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Apple is a fruit produced by \"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "if tokenized_sentence[-1] == 2:\n",
    "    tokenized_sentence = tokenized_sentence[:-1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_token): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence)\n",
    "        sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor]\n",
    "        sentence_embedding = sentence_embedding.unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        with amp.autocast():\n",
    "            prediction = gemma(sentence_embedding)\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "\n",
    "tokens = tokenizer.decode(tokenized_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gemma, 'gemma.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
