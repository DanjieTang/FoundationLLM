{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOzDPUPKjC0W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as opt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6, wandb: bool = False):\n",
        "        super().__init__()\n",
        "        self.sqrt_dim: float = math.sqrt(dim)\n",
        "        self.eps: float = eps\n",
        "        self.wandb: bool = wandb\n",
        "        if wandb:\n",
        "            self.scale = nn.Parameter(torch.ones(dim))\n",
        "            self.bias = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def find_rms_value(self, tensor: torch.Tensor) -> float:\n",
        "        norm_2 = tensor.norm(2, dim=-1)\n",
        "        return norm_2 * self.sqrt_dim\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        tensor = tensor.float() # Using 4 bit float for stability\n",
        "        rms: float = self.find_rms_value(tensor)\n",
        "        tensor = tensor/rms.unsqueeze(-1) + self.eps\n",
        "\n",
        "        if self.wandb:\n",
        "            tensor = tensor * self.scale\n",
        "            tensor = tensor + self.bias\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class ROPEEmbedding(nn.Module):\n",
        "    def __init__(self, max_token: int, dim: int, theta: int):\n",
        "        super().__init__()\n",
        "        self.pos_emb = self.create_embedding(max_token, dim, theta)\n",
        "\n",
        "    def create_embedding(self, max_token: int, dim: int, theta: int) -> torch.Tensor:\n",
        "        tensor = torch.arange(0, dim // 2)\n",
        "        tensor = torch.repeat_interleave(tensor, 2)\n",
        "        tensor = -tensor * 2 / dim\n",
        "        tensor = torch.pow(theta, tensor)\n",
        "\n",
        "        index = torch.arange(max_token).float() # This is the m in the formula\n",
        "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
        "\n",
        "        cos_matrix = tensor.cos()\n",
        "        sin_matrix = tensor.sin()\n",
        "        sin_matrix[0::2] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
        "\n",
        "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
        "        pos_emb = pos_emb.transpose(1, 0)\n",
        "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
        "\n",
        "        return pos_emb\n",
        "\n",
        "    def flip_for_sin(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        original_shape = tensor.shape\n",
        "        tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
        "        tensor = tensor[..., [1, 0]] # Swap\n",
        "        tensor = tensor.reshape(original_shape) # Get back to original shape\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
        "\n",
        "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
        "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
        "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
        "        tensor = cos + sin\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, head_dim: int, q_head: int, kv_head: int, embedding: ROPEEmbedding):\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "        self.q_head = q_head\n",
        "        self.kv_head = kv_head\n",
        "        self.embedding = embedding\n",
        "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
        "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
        "        self.scaler = 1/math.sqrt(hidden_dim)\n",
        "\n",
        "        if q_head != kv_head:\n",
        "            # If we are using multi query attention\n",
        "            assert q_head % kv_head == 0\n",
        "            self.multi_query_attention = True\n",
        "            self.q_kv_scale = q_head//kv_head\n",
        "        else:\n",
        "            self.multi_query_attention = False\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        batch_size, seq_len, hid_dim = tensor.shape\n",
        "\n",
        "        tensor = self.qkv(tensor)\n",
        "        query, key, value = tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
        "\n",
        "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
        "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
        "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
        "\n",
        "        if self.multi_query_attention:\n",
        "            # If we are using multi query attention, duplicate key value heads\n",
        "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
        "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
        "\n",
        "        # Switch to batch_size, head, seq_len, head_dim\n",
        "        query = query.transpose(1, 2)\n",
        "        key = key.transpose(1, 2)\n",
        "        value = value.transpose(1, 2)\n",
        "\n",
        "        # Apply ROPE\n",
        "        query = self.embedding(query)\n",
        "        key = self.embedding(key)\n",
        "\n",
        "        # Classic self attention\n",
        "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
        "        attention_scaled = attention_raw * self.scaler\n",
        "        if attention_mask:\n",
        "            attention_scaled += attention_mask\n",
        "        attention_score = torch.softmax(attention_scaled, dim=1)\n",
        "        value = torch.matmul(attention_score, value)\n",
        "\n",
        "        # Reshape back to batch_size, seq_len, hid_dim\n",
        "        value = value.transpose(1, 2).contiguous()\n",
        "        value = value.view(batch_size, seq_len, hid_dim)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.o(value)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size: int, inner_size: int):\n",
        "        super().__init__()\n",
        "        self.gate_and_up = nn.Linear(hidden_size, inner_size * 2)\n",
        "        self.down = nn.Linear(inner_size, hidden_size)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        tensor = self.gate_and_up(tensor)\n",
        "        gate, up = tensor.chunk(chunks=2, dim=-1)\n",
        "        gate = F.gelu(gate, approximate=\"tanh\")\n",
        "        tensor = gate * up\n",
        "        tensor = self.down(tensor)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GemmaLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, head_dim: int, q_head: int, kv_head: int, embedding: ROPEEmbedding):\n",
        "        super().__init__()\n",
        "        self.norm1 = RMSNorm(hidden_dim)\n",
        "        self.mqa = MultiQueryAttention(hidden_dim, head_dim, q_head, kv_head, embedding)\n",
        "\n",
        "        self.norm2 = RMSNorm(hidden_dim)\n",
        "        self.ffn = FeedForward(hidden_dim, hidden_dim*4)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor):\n",
        "        skip_connection = tensor\n",
        "        tensor = self.norm1(tensor)\n",
        "        tensor = self.mqa(tensor)\n",
        "        tensor += skip_connection\n",
        "\n",
        "        skip_connection = tensor\n",
        "        tensor = self.norm2(tensor)\n",
        "        tensor = self.ffn(tensor)\n",
        "        tensor += skip_connection\n",
        "\n",
        "        return tensor\n",
        "\n",
        "class Gemma(nn.Module):\n",
        "    def __init__(self, num_layer: int, vocab_size: int, max_token: int, hidden_dim: int, head_dim: int, q_head: int, kv_head: int, theta: int = 10000):\n",
        "        super().__init__()\n",
        "        self.embedding = ROPEEmbedding(max_token, head_dim, theta)\n",
        "        self.num_layer = num_layer\n",
        "\n",
        "        self.transformer = nn.ModuleList()\n",
        "        for _ in range(self.num_layer):\n",
        "            self.transformer.append(GemmaLayer(hidden_dim, head_dim, q_head, kv_head, self.embedding))\n",
        "        self.output_norm = RMSNorm(hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, temperature: float = 0.5) -> torch.Tensor:\n",
        "        for layer in self.transformer:\n",
        "            tensor = layer(tensor)\n",
        "\n",
        "        tensor = self.output_norm(tensor)\n",
        "\n",
        "        # Classification\n",
        "        tensor = self.classifier(tensor)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "-ommkUOkjG6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}